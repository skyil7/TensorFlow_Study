# 합성곱 신경망
합성곱 신경망(Convolutional Neural Network)은 1998년 얀 레쿤 교수가 소개한 이후 사용되고 있는 신경망 모델로 이미지 인식 분야에서 특히 강력한 성능을 보여준다.

최근에는 음성인식이나 자연어 처리에도 사용되는 등, 활용성이 매우 뛰어난 성과를 보여주고 있다.

CNN 모델은 기본적으로 컨볼루션 계층과 풀링 계층으로 구성되는데, 이 계층들을 얼마나 많이, 어떠한 방식으로 쌓느냐에 따라 성능이나 기능이 달라진다.

컨볼루션과 풀링 계층의 개념은 꽤나 간단한데, 쉽게 말해 2차원 평면 벡터에서 지정한 영역의 값들을 하나의 값으로 압축하는 것이다.
이 때, 컨볼루션 계층에서는 편향과 가중치를 이용하고, 풀링 계층은 단순히 값들 중 하나를 가져오는 방식을 취한다.

CNN에서 특정 영역을 한 값으로 압축할 때, 압축할 대상이 있는 영역을 윈도우라고 한다. 한 윈도우를 압축한 후에는 영역을 옮겨 다음값을 얻는데, 이 때 다음 값과 이전 값의 간격을 스트라이드라고 한다.

이렇게 입력층의 윈도우를 은닉층의 뉴런 하나로 압축할 때, 컨볼루션 계층에서는 윈도우 크기만큼의 가중치와 1개의 편향을 필요로 하는데, 이때 사용하는 윈도우와 같은 크기의 가중치와 1개의 편향을 아울러 커널(Kernel) 혹은 필터(Filter)라고 한다.

## 일반 신경망과의 차이
위에서 설명한 것이 CNN의 가장 중요한 특징인데, 예를 들어 28*28 사이즈의 MNIST 데이터를 처리할 때, 입력층에서 첫 은닉층으로의 연결은 통상적으로 784개가 필요하다.

그러나 합성곱 신경망에서는 3*3 형태의 가중치 9개만 찾아내면 되기때문에 연산량을 획기적으로 줄일 수 있다.

그러나 이러한 경우엔 복잡한 이미지를 처리하기 힘들어지기 때문에 보통 커널을 여러개 사용한다. 이 커널의 갯수나 크기도 하이퍼 파라미터로써 신경망의 성능을 좌우하는 최적화 요소 중 하나이다.

## 코드 분석
`MNISTwithCNN.py`의 코드를 좀 보자.

아래는 첫 컨볼루션 계층이다.
~~~python
W1 = tf.Variable(tf.random_normal([3,3,1,32],stddev=0.01))#3*3 짜리 가중치
L1 = tf.nn.conv2d(X,W1,strides=[1,1,1,1], padding='SAME')#2d Convolution 
L1 = tf.nn.relu(L1)
~~~
W1 에서 3*3 크기의 가중치를 32개 만들었다.

strides는 각 방향으로 한번에 이동할 크기를 의미하며, padding='SAME'은 커널 슬라이딩 시 이미지의 가장 외과 바깥 한 칸까지 움직이는 옵션으로, 이미지를 조금 더 정확하게 평가할 수 있게 해준다.

아래는 첫 풀링 계층이다.

~~~python
L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME')
~~~
커널 크기를 2*2로 하는 풀링 계층을 만들었다. stride는 2칸으로 했고 역시 위와 같이 padding='SAME'을 주었다.

정확도 99.06%!

## 고수준 API
위 코드를 고오급 API로 컨버팅해봤다.

~~~python
L1 = tf.layers.conv2d(X,32,[3,3],activation= tf.nn.relu, padding='SAME')
L1 = tf.layers.max_pooling2d(L1, [2,2], [2,2], padding='SAME')

L3 = tf.contrib.layers.flatten(L2)
L3 = tf.layers.dense(L3, 256, activation=tf.nn.relu)
~~~

미쳤.... TF는 갓갓입니다.

공부가 끝나면 layers 함수를 더 연구해봐야겠다.